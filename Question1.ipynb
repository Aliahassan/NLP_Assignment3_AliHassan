{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "from scipy import spatial\n",
        "\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "    # Takes in the file path for test file and generate a dictionary\n",
        "    # of question id as the key and the list of question ids similar to it\n",
        "    # as value. It also returns the list of all question ids that have\n",
        "    # at least one similar question\n",
        "    dic_similar_questions = {}\n",
        "    lst_all_test = []\n",
        "    with open(file_path) as fd:\n",
        "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "        for row in rd:\n",
        "            question_id = int(row[0])\n",
        "            lst_similar = list(map(int, row[1:]))\n",
        "            dic_similar_questions[question_id] = lst_similar\n",
        "            lst_all_test.append(question_id)\n",
        "            lst_all_test.extend(lst_similar)\n",
        "    # print(dic_similar_questions)\n",
        "    # print(lst_all_test)\n",
        "    return dic_similar_questions, lst_all_test\n",
        "#read_tsv_test_data(duplicate_questions.tsv\")\n",
        "\n",
        "\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = sentence.split(\" \")\n",
        "  vector = model.wv[words[0]]\n",
        "  for i in range(1, len(words)):\n",
        "    vector += model.wv[words[i]]\n",
        "  return vector/len(words)\n",
        "\n",
        "def train_model(lst_sentences):\n",
        "    model = FastText( window=5, min_n=1)\n",
        "    model.build_vocab(lst_sentences)\n",
        "    model.train(lst_sentences, total_examples=len(lst_sentences), epochs=10)\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    duplicate_file = \"duplicate_questions.tsv\"\n",
        "    post_file = \"Posts_law.xml\"\n",
        "    dic_similar_questions, lst_all_test = read_tsv_test_data(duplicate_file)\n",
        "    post_reader = PostParserRecord(post_file)\n",
        "    lst_training_sentences = []\n",
        "    print(lst_training_sentences)\n",
        "    for question_id in post_reader.map_questions:\n",
        "        if question_id in lst_all_test:\n",
        "            continue\n",
        "        question = post_reader.map_questions[question_id]\n",
        "        title = question.title\n",
        "        body = question.body\n",
        "        # Collect sentences here\n",
        "        for sentence in title.split('.'):\n",
        "            lst_training_sentences.append(sentence.strip())\n",
        "        for sentence in body.split('.'):\n",
        "            lst_training_sentences.append(sentence.strip())\n",
        "        lst_answers = question.answers\n",
        "        if lst_answers is not None:\n",
        "            for answer in lst_answers:\n",
        "                answer_body = answer.body\n",
        "                # Collection sentences here\n",
        "                for sentence in answer_body.split('.'):\n",
        "                    lst_training_sentences.append(sentence.strip())\n",
        "\n",
        "    # train your model\n",
        "    model = train_model(lst_training_sentences)\n",
        "\n",
        "    # This dictionary will have the test question id as the key\n",
        "    # and the most similar question id as the value\n",
        "    dictionary_result = {}\n",
        "\n",
        "     #finding Similar questions using fastText model\n",
        "     for test_question_id in dic_similar_questions:\n",
        "         # for this question you have to find the similar questions\n",
        "         test_question = post_reader.map_questions[test_question_id]\n",
        "         test_title = test_question.title\n",
        "         test_body = test_question.body\n",
        "         max_similarity = 0\n",
        "         most_similar_question_id = -1\n",
        "         for question_id in post_reader.map_questions:\n",
        "             # we are not comparing a question with itself\n",
        "             if question_id == test_question_id:\n",
        "                 continue\n",
        "             question = post_reader.map_questions[question_id]\n",
        "             title = question.title\n",
        "             body = question.body\n",
        "             # use your model and calculate the cosine similarity between the questions\n",
        "             similarity_title = 1 - spatial.distance.cosine(get_sentence_embedding(model, test_title),\n",
        "                                                            get_sentence_embedding(model, title))\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zF8HGS6jrKT",
        "outputId": "90e9fd7c-d611-40fa-b7be-a8212686586e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0]\n"
          ]
        }
      ]
    }
  ]
}